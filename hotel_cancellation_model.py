# -*- coding: utf-8 -*-
"""Hotel_cancellation_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19cSPOjPpygrOi8BwIOPmlbIlXHQEXcRM

Data Source:

```
https://www.kaggle.com/code/marcuswingen/eda-of-bookings-and-ml-to-predict-cancelations
```

## Data Preparation
"""

!pip install opendatasets

"""username	"username" <br>
key	"****"
"""

import opendatasets as od
import pandas as pd
import os
import glob

od.download(
    "https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand/data")

##load data
file_path = "hotel-booking-demand/hotel_bookings.csv"
df = pd.read_csv(file_path)

df.shape ##total records

df.describe()

df.head()

df.info()

"""## EDA"""

df.describe()

df['adults'].value_counts()

#check record that adults = 0 and assign as 1
df.loc[df['adults'] == 0, 'adults'] = 1
df[df['adults'] == 0]

#check null value
#have 4 features that contain null value
df.isnull().sum().sort_values(ascending=False)[:5]

#impute null children with 0 and impute null country with "Not identify"
df['children'].fillna(0, inplace=True)
df['country'].fillna('Not identify', inplace=True)

#change children column type to int instead of float
df['children'] = df['children'].astype('int64')

#check null value again
df.isnull().sum().sort_values(ascending=False)[:5]

#drop those column that hint the cancellation reservation_status and reservation_status_date
df.drop(['reservation_status', 'reservation_status_date'], axis=1, inplace=True)
#drop company and agent; company has null value more than 50% and agent is not that important for the model
df.drop(['company', 'agent'], axis=1, inplace=True)

# add new column call total_guest that sum up all adult + children + baby following DA
df['total_guest'] = df['adults'] + df['babies']

df.info()

#plot graph to see non-cancel and cancel booking. To have a better split for train and test
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

plt.figure(figsize=(8, 6))
ax = sns.countplot(x='is_canceled', data=df, palette='viridis')
plt.title('Distribution of Canceled and Non-Canceled Bookings')
plt.xlabel('Is Canceled (0: No, 1: Yes)')
plt.ylabel('Number of Bookings')
plt.xticks([0, 1], ['Non-Canceled', 'Canceled'])

total = len(df)

# Add count and percentage labels inside each bar (centered)
for p in ax.patches:
    count = int(p.get_height())
    percent = 100 * count / total
    ax.annotate(f'{count:,}\n({percent:.1f}%)',
                (p.get_x() + p.get_width() / 2, p.get_height() / 2),
                ha='center', va='center', color='white', fontsize=12, fontweight='bold')

plt.show()

#create fig of boxplot for numeric column

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
exclude_cols = ['is_canceled']  # add/remove as needed
numeric_cols = [col for col in numeric_cols if col not in exclude_cols]

#Set up the subplot grid size
n_cols = 4
n_rows = (len(numeric_cols) + n_cols - 1) // n_cols # Corrected calculation for n_rows

plt.figure(figsize=(n_cols * 8, n_rows * 4))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.boxplot(data=df, x='is_canceled', y=col)
    plt.title(f'Boxplot of {col} by is_canceled')
    plt.xlabel('Is Canceled (0: No, 1: Yes)')
    plt.tight_layout()

plt.show()
#those that boxplot show identical --> visual hint that these features may not be strong predictors

#show distribution graph for each numeric feature
n_cols = 4
n_rows = (len(numeric_cols) + n_cols - 1) // n_cols

plt.figure(figsize=(n_cols * 5, n_rows * 3))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.tight_layout()

plt.show()

#Remove 5000 ADR outliers
df_no_outlier = df[df['adr'] <= 5000].reset_index(drop=True)

print(f"Removed {len(df) - len(df_no_outlier)} rows with ADR above 5000")

#categorical feature

categorical_cols = [
    "arrival_date_month", "distribution_channel", "market_segment",
    "customer_type", "deposit_type", "hotel", "meal", "country", "reserved_room_type","assigned_room_type"
]

n_cols = 2
n_rows = (len(categorical_cols) + n_cols - 1) // n_cols

plt.figure(figsize=(n_cols * 9, n_rows * 4))
for i, col in enumerate(categorical_cols, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.countplot(data=df_no_outlier, x=col, hue='is_canceled', palette='viridis')
    plt.title(f'Booking Count by {col} and Cancellation Status')
    plt.xlabel(col)
    plt.ylabel('Number of Bookings')
    plt.legend(title='Is Canceled', labels=['No', 'Yes'])
    plt.xticks(rotation=30)
    plt.tight_layout()

plt.show()

categorical_cols = [
    "arrival_date_month", "distribution_channel", "market_segment",
    "customer_type", "deposit_type", "hotel", "meal", "country",
    "reserved_room_type", "assigned_room_type"
]

def plot_cancellation_rate(df, col, ax, top_n=15, min_count=1000):
    # Group rare categories
    value_counts = df[col].value_counts()
    if len(value_counts) > top_n:
        top_categories = value_counts.nlargest(top_n).index
        df = df.copy()
        df[col] = df[col].where(df[col].isin(top_categories), other='Other')

    # Calculate cancellation rate and count per category
    summary = (
        df.groupby(col)['is_canceled']
        .agg(['mean', 'count'])
        .rename(columns={'mean': 'cancel_rate'})
        .sort_values('cancel_rate', ascending=False)
    )

    # Optionally group categories with very low count into 'Other'
    if min_count:
        rare = summary[summary['count'] < min_count].index
        df[col] = df[col].where(~df[col].isin(rare), other='Other')
        summary = (
            df.groupby(col)['is_canceled']
            .agg(['mean', 'count'])
            .rename(columns={'mean': 'cancel_rate'})
            .sort_values('cancel_rate', ascending=False)
        )

    # Plot on the provided axis
    sns.barplot(x=summary.index, y=summary['cancel_rate']*100, palette='viridis', ax=ax)
    ax.set_title(f'Cancellation Rate (%) by {col}')
    ax.set_xlabel(col)
    ax.set_ylabel('Cancellation Rate (%)')
    ax.set_xticklabels(ax.get_xticklabels(), rotation=30)
    ax.set_ylim(0, 100)
    # Annotate with counts
    for i, (rate, count) in enumerate(zip(summary['cancel_rate'], summary['count'])):
        ax.text(i, rate*100, f'n={count}', ha='center', va='bottom', fontsize=9)

# Set up the subplot grid
n_cols = 2
n_rows = (len(categorical_cols) + n_cols - 1) // n_cols
fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 9, n_rows * 4))

# Flatten axes for easy iteration
axes = axes.flatten()

# Plot for each categorical feature
for i, col in enumerate(categorical_cols):
    plot_cancellation_rate(df_no_outlier, col, axes[i], top_n=15, min_count=1000)

# Hide any unused subplots
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

df_no_outlier[df_no_outlier['deposit_type'] == 'Non Refund']['is_canceled'].value_counts(normalize=True)

df_no_outlier[df_no_outlier['deposit_type'] == 'Non Refund']

# Check for any duplicated rows (all columns)
num_duplicates = df_no_outlier.duplicated().sum()
print(f"Number of fully duplicated rows: {num_duplicates}")

# Drop fully duplicated rows, keeping only the first occurrence
df_no_outlier = df_no_outlier.drop_duplicates(keep='first')

print(f"Shape after dropping duplicates: {df_no_outlier.shape}")

#plot graph to see non-cancel and cancel booking. To have a better split for train and test

plt.figure(figsize=(8, 6))
ax = sns.countplot(x='is_canceled', data=df_no_outlier, palette='viridis')
plt.title('Distribution of Canceled and Non-Canceled Bookings')
plt.xlabel('Is Canceled (0: No, 1: Yes)')
plt.ylabel('Number of Bookings')
plt.xticks([0, 1], ['Non-Canceled', 'Canceled'])

total = len(df_no_outlier)

# Add count and percentage labels inside each bar (centered)
for p in ax.patches:
    count = int(p.get_height())
    percent = 100 * count / total
    ax.annotate(f'{count:,}\n({percent:.1f}%)',
                (p.get_x() + p.get_width() / 2, p.get_height() / 2),
                ha='center', va='center', color='white', fontsize=12, fontweight='bold')

plt.show()

#check after drop dup
#with no outlier

numeric_cols = df_no_outlier.select_dtypes(include=['int64', 'float64']).columns.tolist()
exclude_cols = ['is_canceled']  # add/remove as needed
numeric_cols = [col for col in numeric_cols if col not in exclude_cols]

#Set up the subplot grid size
n_cols = 4
n_rows = (len(numeric_cols) + n_cols - 1) // n_cols # Corrected calculation for n_rows

plt.figure(figsize=(n_cols * 8, n_rows * 4))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.boxplot(data=df_no_outlier, x='is_canceled', y=col)
    plt.title(f'Boxplot of {col} by is_canceled')
    plt.xlabel('Is Canceled (0: No, 1: Yes)')
    plt.tight_layout()

plt.show()
#those that boxplot show identical --> visual hint that these features may not be strong predictors

#show distribution graph for each numeric feature
n_cols = 4
n_rows = (len(numeric_cols) + n_cols - 1) // n_cols

plt.figure(figsize=(n_cols * 5, n_rows * 3))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.histplot(df_no_outlier[col], kde=True, bins=30)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.tight_layout()

plt.show()

#Check all outlier
df_no_outlier[df_no_outlier['previous_cancellations'] > 15]

categorical_cols = [
    "arrival_date_month", "distribution_channel", "market_segment",
    "customer_type", "deposit_type", "hotel", "meal", "country",
    "reserved_room_type", "assigned_room_type"
]

def plot_cancellation_rate(df, col, ax, top_n=15, min_count=1000):
    # Group rare categories
    value_counts = df[col].value_counts()
    if len(value_counts) > top_n:
        top_categories = value_counts.nlargest(top_n).index
        df = df.copy()
        df[col] = df[col].where(df[col].isin(top_categories), other='Other')

    # Calculate cancellation rate and count per category
    summary = (
        df.groupby(col)['is_canceled']
        .agg(['mean', 'count'])
        .rename(columns={'mean': 'cancel_rate'})
        .sort_values('cancel_rate', ascending=False)
    )

    # Optionally group categories with very low count into 'Other'
    if min_count:
        rare = summary[summary['count'] < min_count].index
        df[col] = df[col].where(~df[col].isin(rare), other='Other')
        summary = (
            df.groupby(col)['is_canceled']
            .agg(['mean', 'count'])
            .rename(columns={'mean': 'cancel_rate'})
            .sort_values('cancel_rate', ascending=False)
        )

    # Plot on the provided axis
    sns.barplot(x=summary.index, y=summary['cancel_rate']*100, palette='viridis', ax=ax)
    ax.set_title(f'Cancellation Rate (%) by {col}')
    ax.set_xlabel(col)
    ax.set_ylabel('Cancellation Rate (%)')
    ax.set_xticklabels(ax.get_xticklabels(), rotation=30)
    ax.set_ylim(0, 100)
    # Annotate with counts
    for i, (rate, count) in enumerate(zip(summary['cancel_rate'], summary['count'])):
        ax.text(i, rate*100, f'n={count}', ha='center', va='bottom', fontsize=9)

# Set up the subplot grid
n_cols = 2
n_rows = (len(categorical_cols) + n_cols - 1) // n_cols
fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 9, n_rows * 4))

# Flatten axes for easy iteration
axes = axes.flatten()

# Plot for each categorical feature
for i, col in enumerate(categorical_cols):
    plot_cancellation_rate(df_no_outlier, col, axes[i], top_n=15, min_count=1000)

# Hide any unused subplots
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

#export df_no_outlier in csv file
# df_no_outlier.to_csv('df_no_outlier.csv', index=False)

"""## Prepare data"""

#import lib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, roc_auc_score
from xgboost import XGBClassifier

# Separate a small portion of the data as truly unseen data
unseen_data = df_no_outlier.sample(n=1000, random_state=42)
df_for_training = df_no_outlier.drop(unseen_data.index)

display(unseen_data)

df_for_training.columns

"""## No feature selection"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from time import time

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.model_selection import ParameterGrid, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from tqdm import tqdm

# 1. Define target & features
target = "is_canceled"
features = [col for col in df_for_training.columns if col != target]

X = df_for_training[features]
y = df_for_training[[target]]

# 2. Split dataset
# split between cancel and non-cancel equally between test and train dataset (stratify=y)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

# 3. Preprocessing
numeric_features = df_for_training[features].select_dtypes(include=np.number).columns.tolist()
categorical_features = df_for_training[features].select_dtypes(include='object').columns.tolist()

numeric_transformer = StandardScaler() #for outlier RobustScaler
categorical_transformer = OneHotEncoder(handle_unknown="ignore")

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)

# Plot graph to see the distribution of train and test
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Plot for y_train
ax_train = axes[0]
sns.countplot(x=y_train.values.flatten(), palette='viridis', ax=ax_train)
ax_train.set_title('Distribution of Canceled and Non-Canceled Bookings (Training Set)')
ax_train.set_xlabel('Is Canceled (0: No, 1: Yes)')
ax_train.set_ylabel('Number of Bookings')
ax_train.set_xticks([0, 1])
ax_train.set_xticklabels(['Non-Canceled', 'Canceled'])

total_train = len(y_train)
for p in ax_train.patches:
    count = int(p.get_height())
    percent = 100 * count / total_train
    ax_train.annotate(f'{count:,}\n({percent:.1f}%)',
                (p.get_x() + p.get_width() / 2, p.get_height() / 2),
                ha='center', va='center', color='white', fontsize=12, fontweight='bold')

# Plot for y_test
ax_test = axes[1]
sns.countplot(x=y_test.values.flatten(), palette='viridis', ax=ax_test)
ax_test.set_title('Distribution of Canceled and Non-Canceled Bookings (Testing Set)')
ax_test.set_xlabel('Is Canceled (0: No, 1: Yes)')
ax_test.set_ylabel('Number of Bookings')
ax_test.set_xticks([0, 1])
ax_test.set_xticklabels(['Non-Canceled', 'Canceled'])

total_test = len(y_test)
for p in ax_test.patches:
    count = int(p.get_height())
    percent = 100 * count / total_test
    ax_test.annotate(f'{count:,}\n({percent:.1f}%)',
                (p.get_x() + p.get_width() / 2, p.get_height() / 2),
                ha='center', va='center', color='white', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

X_train.shape, X_test.shape

"""## For feature selection"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.model_selection import ParameterGrid, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from tqdm import tqdm

# For Feature Selection
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

# Define a helper function to get classification report as a DataFrame
def get_classification_report_df(y_true, y_pred, model_name):
    report = classification_report(y_true, y_pred, output_dict=True)
    df_report = pd.DataFrame(report).transpose()
    df_report = df_report.drop(columns=['support'])
    try:
        auc_roc_val = roc_auc_score(y_true, y_pred) # Assuming y_pred are class labels for simplicity here
    except ValueError:
        auc_roc_val = np.nan # If roc_auc_score fails, set to NaN
    df_report.loc['auc_roc'] = auc_roc_val
    df_report.rename(columns={'precision': f'{model_name}_precision', 'recall': f'{model_name}_recall', 'f1-score': f'{model_name}_f1-score'}, inplace=True)
    return df_report

# Define the target variable
target = "is_canceled"
features = [col for col in df_for_training.columns if col != target]

# Dynamically determine numeric and categorical features
numeric_features = df_for_training[features].select_dtypes(include=np.number).columns.tolist()
categorical_features = df_for_training[features].select_dtypes(include='object').columns.tolist()


# Split train test
X = df_for_training[features]
y = df_for_training[target]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y #split data equally between non-cancel Vs canceled
)

# Instantiate transformers
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown="ignore")

# Create a ColumnTransformer named preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Keep other columns if any, though our 'features' list is exhaustive
)

print("Preprocessing pipeline (ColumnTransformer) defined successfully.\n")

# --- Feature Selection using RandomForestClassifier ---
print("--- Performing Feature Selection ---")

# Create a pipeline for feature selection
fs_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('clf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))
])

# Fit the pipeline to the training data
fs_pipeline.fit(X_train, y_train)

# Use SelectFromModel to choose features based on feature importances
selector = SelectFromModel(fs_pipeline.named_steps['clf'], threshold='median', prefit=True) # or a numerical threshold

# Transform X_train to get feature names after one-hot encoding
X_train_processed_temp = fs_pipeline.named_steps['preprocessor'].fit_transform(X_train)

preprocessed_feature_names = numeric_features
# For categorical features
if fs_pipeline.named_steps['preprocessor'].transformers_[-1][1].get_feature_names_out is not None:
    preprocessed_feature_names.extend(fs_pipeline.named_steps['preprocessor'].transformers_[-1][1].get_feature_names_out(categorical_features))
else:
    print("Warning: get_feature_names_out not available. Feature names might be generic for OHE.")
    dummy_encoder = OneHotEncoder(handle_unknown="ignore").fit(X_train[categorical_features])
    preprocessed_feature_names.extend([f'cat_{col}_{val}' for col in categorical_features for val in dummy_encoder.categories_[categorical_features.index(col)]])

# Select features from the processed data
X_train_selected = selector.transform(X_train_processed_temp)

# Get the names of the selected features
selected_feature_indices = selector.get_support(indices=True)
selected_features_names = [preprocessed_feature_names[i] for i in selected_feature_indices]

print(f"Selected {len(selected_features_names)} features out of {len(preprocessed_feature_names)}:")
print(selected_features_names)

# Create a new preprocessor that only includes the selected features (original feature names)
selected_numeric_features = [f for f in numeric_features if f in ' '.join(selected_features_names)] # Simplified check
selected_categorical_features = [f for f in categorical_features if f in ' '.join(selected_features_names)] # Simplified check

# Refine the list of selected original features more accurately
final_selected_original_features = []
for original_feature in features:
    if original_feature in numeric_features:
        # Check if the numeric feature itself was selected or if its importance led to selection
        if any(f.startswith(original_feature) for f in selected_features_names): # Check if it's in the selected list (exact match or start of OHE)
            final_selected_original_features.append(original_feature)
    elif original_feature in categorical_features:
        # For categorical, if any of its one-hot encoded columns are selected, keep the original categorical feature
        if any(f.startswith(f'cat_{original_feature}_') for f in selected_features_names):
             final_selected_original_features.append(original_feature)

print("Original features corresponding to selected preprocessed features:")
print(final_selected_original_features)

# Update the global features variable and re-split X, y
features = final_selected_original_features
numeric_features = [f for f in features if f in df_for_training.select_dtypes(include=np.number).columns]
categorical_features = [f for f in features if f in df_for_training.select_dtypes(include='object').columns]

X = df_for_training[features]
y = df_for_training[target]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

# Re-define the preprocessor with only the selected original features
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
    ],
    remainder='drop' # Drop columns not in the selected features
)

print("Updated Preprocessing pipeline (ColumnTransformer) defined with selected features.\n")

#plot graph to show feature importance
feature_importances = pd.Series(fs_pipeline.named_steps['clf'].feature_importances_, index=preprocessed_feature_names)
feature_importances.nlargest(20).plot(kind='barh')
plt.show()

print(X_train.columns)
print(X_train.shape)

"""#Run All Model Training, Optimization, and Evaluation

## KNN
"""

from time import time
from sklearn.metrics import recall_score, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score, ParameterGrid

# --- KNN Model ---
print("--- Training and Optimizing KNN Model ---")
knn_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", KNeighborsClassifier())
])

# Prepare Param for GridSearchCV
param_grid_knn = {
    'classifier__n_neighbors': [3, 5, 10, 15, 20]
}

param_grid = list(ParameterGrid(param_grid_knn))

best_score = -1
best_params = None

# GridSearch CV
for i, params in enumerate(param_grid, start=1):
    print(f"\n=== Param set {i}/{len(param_grid)}: {params}")
    t0 = time()
    model = knn_pipeline.set_params(**params)

    scores = cross_val_score(
        model,
        X_train,
        y_train.values.ravel(),
        cv=5,
        scoring="recall",
        n_jobs=1
    )
    print(f"Finished in {time() - t0:.1f} seconds")
    mean_score = scores.mean()

    if mean_score > best_score:
        best_score = mean_score
        best_params = params

print("Best parameters for KNN found:", best_params)
print("Best Recall score for KNN:", best_score)

# --- Evaluating Best KNN Model Performance ---
best_knn_model_final = knn_pipeline.set_params(**best_params)
best_knn_model_final.fit(X_train, y_train.values.ravel())

y_train_pred_best = best_knn_model_final.predict(X_train)
y_test_pred_best = best_knn_model_final.predict(X_test)

train_recall = recall_score(y_train, y_train_pred_best)
test_recall = recall_score(y_test, y_test_pred_best)
train_accuracy = accuracy_score(y_train, y_train_pred_best)
test_accuracy = accuracy_score(y_test, y_test_pred_best)

print(f"\nBest KNN Model Performance (using best_params={best_params}):")
print(f"Train Recall: {train_recall:.4f}")
print(f"Test Recall: {test_recall:.4f}")
print(f"Train Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

conf_matrix_train = confusion_matrix(y_train, y_train_pred_best)
conf_matrix_test = confusion_matrix(y_test, y_test_pred_best)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

sns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Non-Canceled', 'Predicted Canceled'],
            yticklabels=['Actual Non-Canceled', 'Actual Canceled'], ax=axes[0])
axes[0].set_xlabel('Predicted Label')
axes[0].set_ylabel('True Label')
axes[0].set_title('Confusion Matrix for Best KNN Model (Train Set)')

sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Non-Canceled', 'Predicted Canceled'],
            yticklabels=['Actual Non-Canceled', 'Actual Canceled'], ax=axes[1])
axes[1].set_xlabel('Predicted Label')
axes[1].set_ylabel('True Label')
axes[1].set_title('Confusion Matrix for Best KNN Model (Test Set)')

plt.tight_layout()
plt.show()

# --- Train-Test Curve for KNN ---
print("\n--- Generating Train-Test Curve for KNN (varying n_neighbors) ---")
n_neighbors_range = [3, 5, 10, 15, 20, 25, 30]

train_accuracy_knn = []
test_accuracy_knn = []
train_recall_knn = []
test_recall_knn = []

for n in tqdm(n_neighbors_range, desc="Calculating KNN scores for curve"):
    knn_temp_pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", KNeighborsClassifier(n_neighbors=n))
    ])
    knn_temp_pipeline.fit(X_train, y_train.values.ravel())

    y_train_pred = knn_temp_pipeline.predict(X_train)
    y_test_pred = knn_temp_pipeline.predict(X_test)

    train_accuracy_knn.append(accuracy_score(y_train, y_train_pred))
    test_accuracy_knn.append(accuracy_score(y_test, y_test_pred))
    train_recall_knn.append(recall_score(y_train, y_train_pred))
    test_recall_knn.append(recall_score(y_test, y_test_pred))

# Plotting the train-test accuracy curve
plt.figure(figsize=(10, 6))
plt.plot(n_neighbors_range, train_accuracy_knn, label='Training Accuracy', marker='o')
plt.plot(n_neighbors_range, test_accuracy_knn, label='Testing Accuracy', marker='o')
plt.title('KNN: Training vs. Testing Accuracy (Train-Test Curve for n_neighbors)')
plt.xlabel('Number of Neighbors (n_neighbors)')
plt.ylabel('Accuracy')
plt.grid(True)
plt.legend()
plt.show()

# Plotting the train-test recall curve
plt.figure(figsize=(10, 6))
plt.plot(n_neighbors_range, train_recall_knn, label='Training Recall', marker='o')
plt.plot(n_neighbors_range, test_recall_knn, label='Testing Recall', marker='o')
plt.title('KNN: Training vs. Testing Recall (Train-Test Curve for n_neighbors)')
plt.xlabel('Number of Neighbors (n_neighbors)')
plt.ylabel('Recall')
plt.grid(True)
plt.legend()
plt.show()

# Report
print("\nClassification Report for Best KNN Model:")
print(classification_report(y_test, y_test_pred_best))
print("AUC-ROC on Test Set for Best KNN Model:", roc_auc_score(y_test, y_test_pred_best))

conf_matrix_xgb = confusion_matrix(y_test, y_test_pred_best)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Non-Canceled', 'Predicted Canceled'],
            yticklabels=['Actual Non-Canceled', 'Actual Canceled'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Best KNN Model')
plt.show()

# Process the unseen data using the same preprocessor as the model
unseen_data_processed = preprocessor.transform(unseen_data[features])

# Make predictions (class labels) using the best XGBoost model
unseen_data_predictions_knn = best_knn_model_final.predict(unseen_data[features]) # Use the original features of unseen_data for pipeline

# Get prediction probabilities for AUC-ROC if needed, although for this display, class labels are enough
unseen_data_probabilities_knn = best_knn_model_final.predict_proba(unseen_data[features])[:, 1]

# Display unseen data with predictions and actual values
unseen_data_with_predictions_knn = unseen_data.copy()
unseen_data_with_predictions_knn['predicted_is_canceled'] = unseen_data_predictions_knn
unseen_data_with_predictions_knn['predicted_probability_canceled'] = unseen_data_probabilities_knn

print("Unseen Data with Predictions:")
display(unseen_data_with_predictions_knn[['is_canceled', 'predicted_is_canceled', 'predicted_probability_canceled'] + features])

# Optionally, you can calculate performance metrics for the unseen data if you have ground truth
print("\nPerformance on Unseen Data:")
print(classification_report(unseen_data['is_canceled'], unseen_data_predictions_knn))
print("AUC-ROC on Unseen Data:", roc_auc_score(unseen_data['is_canceled'], unseen_data_probabilities_knn))

"""## XGBoost"""

# --- XGBoost Model ---
print("\n--- Training and Optimizing XGBoost Model ---")

# Pipeline
xgb_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", XGBClassifier(
        random_state=42,
        eval_metric="logloss",   # Use logloss for binary classification
        tree_method="hist",      # much faster on CPU
        n_jobs=1                 # avoid CPU oversubscription with CV
    ))
])

# Hyperparameter grid
param_grid_xgb = {
    'classifier__n_estimators': [100, 200, 300, 400, 500, 600, 700],
    'classifier__learning_rate': [0.01, 0.05, 0.1],
    'classifier__max_depth': [1, 3, 5, 7, 9, 11, 13]
}

param_grid = list(ParameterGrid(param_grid_xgb))

best_score = -1
best_params = None

# Manual CV with progress bar
for params in tqdm(param_grid, desc="XGBoost grid search"):
    model = xgb_pipeline.set_params(**params)

    scores = cross_val_score(
        model,
        X_train, y_train,
        cv=5,
        scoring="recall",
        n_jobs=-1
    )
    mean_score = scores.mean()

    if mean_score > best_score:
        best_score = mean_score
        best_params = params

print("Best params:", best_params)
print("Best Recall:", best_score)

# --- Generate Train-Test Curve for XGBoost ---
print("\n--- Generating Train-Test Curve for XGBoost (varying n_estimators) ---")
# Use best_params from GridSearchCV for fixed hyperparameters
# Fallback to default if best_params is None (e.g., if GridSearchCV was skipped/interrupted)
best_learning_rate = best_params.get('classifier__learning_rate', 0.1) if best_params else 0.1
best_max_depth = best_params.get('classifier__max_depth', 7) if best_params else 7

n_estimators_range = range(50, 801, 50) # Test a range around the typical optimal n_estimators

train_scores_xgb = []
test_scores_xgb = []

for n_est in tqdm(n_estimators_range, desc="Calculating XGBoost scores for curve"):
    xgb_temp_pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", XGBClassifier(
            n_estimators=n_est,
            learning_rate=best_learning_rate, # Use best learning rate
            max_depth=best_max_depth, # Use best max_depth
            random_state=42,
            eval_metric="logloss", # binary classification
            tree_method="hist",
            n_jobs=-1
        ))
    ])

    xgb_temp_pipeline.fit(X_train, y_train)

    y_train_pred_xgb = xgb_temp_pipeline.predict(X_train)
    train_scores_xgb.append(accuracy_score(y_train, y_train_pred_xgb))

    y_test_pred_xgb = xgb_temp_pipeline.predict(X_test)
    test_scores_xgb.append(accuracy_score(y_test, y_test_pred_xgb))

# Plotting the train-test curve
plt.figure(figsize=(10, 6))
plt.plot(n_estimators_range, train_scores_xgb, label='Training Accuracy', marker='o')
plt.plot(n_estimators_range, test_scores_xgb, label='Testing Accuracy', marker='o')
plt.title('XGBoost: Training vs. Testing Accuracy (Train-Test Curve for n_estimators)')
plt.xlabel('Number of Estimators (n_estimators)')
plt.ylabel('Accuracy')
plt.grid(True)
plt.legend()
plt.show()

print("\n--- Interpreting the Train-Test Curve ---")
print("Observe the gap between the Training Accuracy and Testing Accuracy curves:")
print("- If Training Accuracy is high but Testing Accuracy is significantly lower, the model is likely overfitting.")
print("- If both Training and Testing Accuracy are low and close together, the model is likely underfitting.")
print("- The sweet spot is where both accuracies are high and the gap between them is minimal.")

best_xgb_model_final = xgb_pipeline.set_params(**best_params)
best_xgb_model_final.fit(X_train, y_train)

# Predict on test data
y_test_pred_xgb = best_xgb_model_final.predict(X_test)
y_test_prob_xgb = best_xgb_model_final.predict_proba(X_test)[:, 1]

# Report
print("\nClassification Report for Best XGBoost Model:")
print(classification_report(y_test, y_test_pred_xgb))
print("AUC-ROC on Test Set for Best XGBoost Model:", roc_auc_score(y_test, y_test_prob_xgb))

conf_matrix_xgb = confusion_matrix(y_test, y_test_pred_xgb)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Non-Canceled', 'Predicted Canceled'],
            yticklabels=['Actual Non-Canceled', 'Actual Canceled'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Best XGBoost Model')
plt.show()

# Process the unseen data using the same preprocessor as the model
unseen_data_processed = preprocessor.transform(unseen_data[features])

# Make predictions (class labels) using the best XGBoost model
unseen_data_predictions = xgb_temp_pipeline.predict(unseen_data[features]) # Use the original features of unseen_data for pipeline

# Get prediction probabilities for AUC-ROC if needed, although for this display, class labels are enough
unseen_data_probabilities = xgb_temp_pipeline.predict_proba(unseen_data[features])[:, 1]

# Display unseen data with predictions and actual values
unseen_data_with_predictions = unseen_data.copy()
unseen_data_with_predictions['predicted_is_canceled'] = unseen_data_predictions
unseen_data_with_predictions['predicted_probability_canceled'] = unseen_data_probabilities

print("Unseen Data with Predictions:")
display(unseen_data_with_predictions[['is_canceled', 'predicted_is_canceled', 'predicted_probability_canceled'] + features])

# Optionally, you can calculate performance metrics for the unseen data if you have ground truth
print("\nPerformance on Unseen Data:")
print(classification_report(unseen_data['is_canceled'], unseen_data_predictions))
print("AUC-ROC on Unseen Data:", roc_auc_score(unseen_data['is_canceled'], unseen_data_probabilities))

"""## Logistic"""

# --- Logistic Regression Model ---
print("\n--- Training and Optimizing Logistic Regression Model ---")

# Pipeline
lr_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", LogisticRegression(
        random_state=42,
        solver="lbfgs",  #good default for L2
        max_iter=1000
    )),
])

# Prepare Param for GridSearchCV
param_grid_lr = {
    'classifier__C': [0.01, 0.1, 1, 10, 100],
    'classifier__penalty': ['l1', 'l2']
}

grid_search_lr = GridSearchCV(
    lr_pipeline,
    param_grid_lr,
    cv=5,
    scoring='recall',
    n_jobs=-1,
    verbose=1
)

# Fit Model
grid_search_lr.fit(X_train, y_train)

print("Best parameters for Logistic Regression found:", grid_search_lr.best_params_)
print("Best Recall score for Logistic Regression:", grid_search_lr.best_score_)

# --- Generate Train-Test Curve for Logistic Regression (varying C) ---
print("\n--- Generating Train-Test Curve for Logistic Regression (varying C) ---")

# Use best_params from GridSearchCV for fixed hyperparameters
best_penalty = grid_search_lr.best_params_.get('classifier__penalty', 'l2')

# If the best penalty is 'l1', ensure the solver supports it. 'liblinear' supports both.
# If 'lbfgs' is chosen with 'l1' penalty, it will fail. So, adjusting solver.
solver_for_curve = "liblinear" if best_penalty == 'l1' else "lbfgs"

C_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000] # Extended range for C values

train_scores_lr = []
test_scores_lr = []

for c_val in tqdm(C_range, desc="Calculating Logistic Regression scores for curve"):
    lr_temp_pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", LogisticRegression(
            C=c_val,
            penalty=best_penalty,
            solver=solver_for_curve,
            random_state=42,
            max_iter=1000
        ))
    ])

    lr_temp_pipeline.fit(X_train, y_train)

    y_train_pred_lr_curve = lr_temp_pipeline.predict(X_train)
    train_scores_lr.append(accuracy_score(y_train, y_train_pred_lr_curve))

    y_test_pred_lr_curve = lr_temp_pipeline.predict(X_test)
    test_scores_lr.append(accuracy_score(y_test, y_test_pred_lr_curve))

# Plotting the train-test curve
plt.figure(figsize=(10, 6))
plt.plot(C_range, train_scores_lr, label='Training Accuracy', marker='o')
plt.plot(C_range, test_scores_lr, label='Testing Accuracy', marker='o')
plt.xscale('log') # C is often varied on a log scale
plt.title('Logistic Regression: Training vs. Testing Accuracy (Train-Test Curve for C)')
plt.xlabel('C (Inverse of Regularization Strength)')
plt.ylabel('Accuracy')
plt.grid(True)
plt.legend()
plt.show()

print("\n--- Interpreting the Train-Test Curve ---")
print("Observe the gap between the Training Accuracy and Testing Accuracy curves:")
print("- If Training Accuracy is high but Testing Accuracy is significantly lower, the model is likely overfitting.")
print("- If both Training and Testing Accuracy are low and close together, the model is likely underfitting.")
print("- The sweet spot is where both accuracies are high and the gap between them is minimal.")

# Confusion matrix and classification report for test
best_lr_model = grid_search_lr.best_estimator_
y_pred_lr = best_lr_model.predict(X_test)

# Report
print("\nClassification Report for Best XGBoost Model:")
print(classification_report(y_test, y_pred_lr))
print("AUC-ROC on Test Set for Best XGBoost Model:", roc_auc_score(y_test, y_pred_lr))

conf_matrix_xgb = confusion_matrix(y_test, y_pred_lr)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Non-Canceled', 'Predicted Canceled'],
            yticklabels=['Actual Non-Canceled', 'Actual Canceled'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Best XGBoost Model')
plt.show()

# Process the unseen data using the same preprocessor as the model
unseen_data_processed = preprocessor.transform(unseen_data[features])

# Make predictions (class labels) using the best XGBoost model
unseen_data_predictions = lr_temp_pipeline.predict(unseen_data[features]) # Use the original features of unseen_data for pipeline

# Get prediction probabilities for AUC-ROC if needed, although for this display, class labels are enough
unseen_data_probabilities = lr_temp_pipeline.predict_proba(unseen_data[features])[:, 1]

# Display unseen data with predictions and actual values
unseen_data_with_predictions = unseen_data.copy()
unseen_data_with_predictions['predicted_is_canceled'] = unseen_data_predictions
unseen_data_with_predictions['predicted_probability_canceled'] = unseen_data_probabilities

print("Unseen Data with Predictions:")
display(unseen_data_with_predictions[['is_canceled', 'predicted_is_canceled', 'predicted_probability_canceled'] + features])

# Optionally, you can calculate performance metrics for the unseen data if you have ground truth
print("\nPerformance on Unseen Data:")
print(classification_report(unseen_data['is_canceled'], unseen_data_predictions))
print("AUC-ROC on Unseen Data:", roc_auc_score(unseen_data['is_canceled'], unseen_data_probabilities))